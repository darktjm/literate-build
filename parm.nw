% -*- mode: Noweb; noweb-code-mode: c-mode; -*-
% Build with noweb:
%  notangle -t8 build.nw > makefile
%  make
% or:  see build section below.
%%% requires build
\documentclass[twoside,english]{article}
\usepackage[letterpaper,rmargin=1.5in,bmargin=1in]{geometry}
\usepackage{noweb}
\noweboptions{smallcode,longchunks}
%%% latex preamble
\usepackage{hyperref}

\begin{document}

\title{Parameterized Chunks for noweb}
\author{Thomas J. Moore}
\date{Version 2.0\\21 September, 2021}
\maketitle

This document is placed in the public domain in 2021 by Thomas J. Moore.

\tableofcontents

\section{Introduction}

I like to consolidate code as much as possible.  I do this in C using
either subroutines or preprocessor macros.  The macros are more useful
when the number of inputs and/or outputs are large, or using a macro
is significantly faster.  The subroutines cannot even be used when
doing something like defining a large number of similar symbols, or
providing a more convenient way to define data tables.  The
preprocessor can usually accomplish what I need, but the price is that
the debugger cannot debug code defined by the preprocessor.  In
addition, there are some languages which lack a preprocessor.  As
such, the original Web system had macros with arguments, and I would
like something similar.

% Begin-doc nw-enh-syntax

I propose the following syntax changes:

\begin{itemize}

\item A chunk name used in a chunk definition line may contain
parameter definitions in parentheses.  All parameter names begin with
an at-sign (@).  The only characters forbidden in parameter names are
close-parentheses and invalid noweb syntax.  Thus the parameter name
is terminated by its closing parenthesis.  Parameter definition syntax
within in-line code is ignored.

For example:
\begin{quote}
\begin{verbatim}
@<<macro (@arg1) with -(@arg2)- (but not @[[(@arg3)]])>>=
text
@@
\end{verbatim}
\end{quote}

The parameters are [[@arg1]] and [[@arg2]].

\item No chunk name may begin with an at-sign (@).  That is, the names
starting with an at-sign are reserved for parameter names.

\item Chunk references beginning with an at-sign (@) expand to the
parameter with that name, as defined by the closest parent taking a
parameter of that name.  When weaved, such references are not linked
anywhere, and they are not added to the index.  When tangled, the
[[-L]] option is ignored.

For example:

\begin{quote}
\begin{verbatim}
@<<macro (@arg)>>=
text @<<@arg>>
@<<other>>
@@
@<<macro2 (@arg)>>=
text2 @<<@arg>>
@<<other>>
@@
@<<other>>=
@<<@arg>>
@@
\end{verbatim}
\end{quote}

When expanded within [[macro]], both [[@arg]] and [[other]] will
expand using [[macro]]'s argument.  When expanded within [[macro2]],
both [[@arg]] and [[other]] will expand using [[macro2]]'s argument.

\item A chunk reference consisting of the name of a chunk taking
parameters, with all parameter definitions replaced by noweb in-line
code segments, represents an expansion of the reference with the
parameters taking the value of the code within the in-line code
segments in their respective positions.  No expansion is done within
the code segments except for parameter references; these may be used
to pass parameters on to another macro.

For example:

\begin{quote}
\begin{verbatim}
@<<macro (@arg)>>=
text @<<@arg>>
@@
@<<macro2 (@arg)>>=
@<<macro @[[and @<<@arg>>]]>>
@@
@<<macro3 (@arg)>>=
@<<macro4>>
@@
@<<macro4>>=
can reference @<<@arg>> of macro3.
@@
@<<caller>>=
text @<<macro @[[text2]]>> @<<macro2 @[[text3]]>>
@<<macro3 @[[text4]]>>
@@
\end{verbatim}
\end{quote}

The [[caller]] chunk expands to

\begin{quote}
\begin{verbatim}
text text text2 text and text3
can reference text4 of macro3.
\end{verbatim}
\end{quote}

\item When more than one parameterized chunk could be used to replace
a reference, the one with the least parameter replacements is chosen.
For equal numbers, the one which does not contain the earliest
parameter replacement is chosen.  This allows for specializtion,
although it is not possible to inplicitly pass specialized parameters
down with the original parameter name.

For example:

\begin{quote}
\begin{verbatim}
@<<macro (@arg1) and (@arg2)>>=
text @<<@arg1>> blah @<<@arg2>>
@@
@<<macro @[[1]] and (@arg)>>=
text blah blah @<<@arg>>
@@
@<<macro (@arg) and @[[2]]>>=
text blah blah blah @<<@arg>>
@@
@<<*>>=
@<<macro @[[a]] and @[[b]]>> -- one blah
@<<macro @[[1]] and @[[b]]>> -- two blahs
@<<macro @[[a]] and @[[2]]>> -- three blahs
@<<macro @[[1]] and @[[2]]>> -- two blahs and a warning
@@
\end{verbatim}
\end{quote}

\item For literal text matching, all passed-on variable references are
expanded first.

For example:

\begin{quote}
\begin{verbatim}
@<<macro (@arg)>>=
1 @<<@arg>>
@@
@<<macro @[[x y]]>>=
1 z
@@
@<<macro2 (@arg)>>=
@<<macro @[[x @<<@arg>>]]>>
@@
@<<*>>=
@<<macro2 @[[x]]>> -- 1 x x
@<<macro2 @[[y]]>> -- 1 z
@@
\end{verbatim}
\end{quote}

\end{itemize}
% End-doc nw-enh-syntax

\section{Tangling}

Rather than create an entirely new tangler, the noweb pipeline is
used.  Also, rather than modifying the noweb tangler, a filter is
created.  A filter is less efficient than simply replacing the
tangler, but it is compatible with both noweb 2 and 3.

While I would prefer to write this in C, I have decided to use C++
instead.  The main advantage is that C++ has usable variable-sized
strings, arrays and hash tables in the standard, whereas I'd have to
reimplement these or use something like GLib (which is not easy to
locate on systems that don't have [[pkg-config]]) or noweb's code
(which does things in ways I don't particularly like).  Even C++ has
issues with respect to whether or not the STL is usable at all, or
what standard the installed compiler complies with.  My system is
modern and up-to-date, but I have at least tried to make this
compatible with C++-98 through C++-20 without errors or warnings.

%\lstset{language=C++}
<<Common Parm Filter Prefix>>=
/***************************
  GENERATED FILE:  DO NOT EDIT OR READ THIS FILE
  Instead, read or edit the noweb file(s) from which this was generated.
****************************/
#include <vector>
#include <string>
#include <iostream>
#include <map>
extern "C" {
#include <stdlib.h>
}
#if __cplusplus < 201100L
#define unordered_map map
#else
#include <unordered_map>
#endif

using namespace std;
typedef unordered_map<string, string> strmap;
@

<<nt-parm.cpp>>=
<<Common Parm Filter Prefix>>

<<[[nt-parm]] local definitions>>

int main(int argc, const char **argv)
{
  <<Parse [[nt-parm]] command-line options>>

  <<Gather code chunks from noweb pipeline>>

  <<Mangle and dump pipeline>>
  return 0;
}
@

<<Parse [[nt-parm]] command-line options>>=
<<[[nt-parm]] Command-line Option Storage>>
const char *progname = *argv;
bool valid = true;
while(--argc > 0 && **++argv == '-') {
  switch(argv[0][1]) {
    <<[[nt-parm]] Command Line Option Case>>
    default:
      valid = false;
      break;
  }
}
if(!valid || argc > 0) {
  cerr << "Invalid option: " << *argv << '\n';
  cerr << "Usage:  " << progname << "[options]; Options:\n"
  <<[[nt-parm]] Flag Usage>>;
  exit(1);
}
@

Tangling a parameterized chunk is done by duplicating the chunk
definition for every unique reference, replacing parameters as
necessary.  Since the above definition allows chunks expanded within
the parameterized chunk (recursively) to also expand the same
parameters, they may need to be duplicated as well.  Chunk references
in the unduplicated code do not need to be modified, since the newly
created chunk names match their usage.  In fact, the original input
can be safely dumped unmodified to the output while it's being read,
leaving only the duplicated/modified chunks for the
[[<<Mangle and dump pipeline>>]] phase.

Each code chunk, bracketed by [[@begin code]] .. [[@end code]], is
slurped into an array of text ([[codetext]]).  The array index of
[[codetext]] is not incremented every line, but instead only on
"interesting" lines.  In order to keep error messages accurate, the
line and file information is tracked as well ([[lineno]], [[file]]),
using the [[@file]], [[@line]], [[@nl]], and [[@index nl]] directives.

<<[[nt-parm]] local definitions>>=
vector<string> codetext;
@

<<Gather code chunks from noweb pipeline>>=
<<Prepare to gather code chunks from noweb pipeline>>
string line;
int lineno = 1;
string file = "<stdin>";
bool incode = false;
codetext.push_back("");
while(getline(cin, line).good() || line.size()) {
  <<Print [[line]]>>
  if(!line_cmp_prefix_sp_or_end("@begin code")) {
    incode = true;
    codetext.back() += line + '\n';
  } else if(!line_cmp_prefix_sp_or_end("@end code")) {
    codetext.back() += line + '\n';
    <<Finish off reading code chunk>>
    codetext.push_back("");
    incode = false;
  } else {
    if(!line.cmp_prefix("@line "))
      lineno = stoi(line.substr(6)) - 1;
    else if(!line.cmp_prefix("@file ")) {
      file = tail(line, 6);
      lineno = 1;
    } else if(!line_cmp_prefix_sp_or_end("@nl") ||
            !line_cmp_prefix_sp_or_end("@index nl"))
      lineno++;
    <<[[else]] process other interesting markup lines>>
    <<Tack [[line]] to [[codetext.back()]]>>
  }
}
@

<<Common Parm Filter Prefix>>=
#define cmp_prefix(s) compare(0, sizeof(s) - 1, s)
#define line_cmp_prefix_sp_or_end(s) \
  (line.cmp_prefix(s) || \
   (line.size() >= sizeof(s) && line[sizeof(s) - 1] != ' '))
#define tail(s, n) s.substr(n, s.size() - n)
@

Although we're only interested in code chunks, [[nountangle]] requires
documentation chunks, as well.  I don't feel that code should be
generated this way, since it discourages reading of the prepared
documentation, but there is little reason not to support it.  A
command-line option is provided to support this.

<<[[nt-parm]] Command-line Option Storage>>=
bool slurp_doc = false;
@

<<[[nt-parm]] Command Line Option Case>>=
case 'd':  slurp_doc = true;  break;
@

<<[[nt-parm]] Flag Usage>>=
"\t-d Include preceeding document chunk with generated code chunks\n"
@

<<Tack [[line]] to [[codetext.back()]]>>=
if(incode || slurp_doc)
  codetext.back() += line + '\n';
@

As stated above, it's perfectly safe to dump lines immediately, since
the primary job is to append new code chunks.  However, it may be
desirable to not dump parameterized chunks that are referenced with
parameters, but aren't directly referenced.  A command-line option is
provided to support this.%
\footnote{OK, I'm being obtuse about this, but really, this is for
[[noroots]].  I suppose a [[noweb]]-like utility that extracts roots
would benefit as well.}

<<[[nt-parm]] Command-line Option Storage>>=
bool rmunref = false;
@

<<[[nt-parm]] Command Line Option Case>>=
case 'r':  rmunref = true;  break;
@

<<[[nt-parm]] Flag Usage>>=
"\t-r Remove directly unused chunks if they are used with parms\n"
@

<<Print [[line]]>>=
if(!rmunref)
  cout @<< line << '\n';
@

Since we only care about chunk definition ([[@defn]]) and reference
([[@use]]) lines, array indices are only incremented when such lines
are found.  An alternate appropach would've been to store each chunk
as a single array element, with offsets to the definition and use
lines stored separately, but the approach used here reduces the number
of extra data structures and simplifies the text replacement that
needs to be done for the duplicate chunks.

<<Prepare to gather code chunks from noweb pipeline>>=
string curdef;
@

<<[[else]] process other interesting markup lines>>=
else if(!line.cmp_prefix("@defn ")) {
  if(!incode) {
    cerr @<< file @<< ':' @<< lineno @<< ": invalid chunk syntax\n";
    exit(1);
  }
  if(line[6] == '@') {
    cerr @<< file @<< ':' @<< lineno @<< ": "
            "initial @ is reserved for parameter names\n";
    cout @<< "@nl\n@end code\n"; // prevent malformed code messages
    cout @<< "@fatal invalid chunk name\n";
    exit(1);
  }
  codetext.back() += line + '\n';
  <<Store [[codetext]] file and line info>>
  curdef = tail(line, 6);
  <<Store [[curdef]] info>>
  codetext.push_back("");
  continue; // skip adding line to codetext
} else if(!line.cmp_prefix("@use ")) {
  if(!curdef.empty()) {
    if(codetext.back().size())
      codetext.push_back("");
    codetext.back() += line + '\n';
    <<Store [[codetext]] file and line info>>
    <<Store new [[@use]]>>
    codetext.push_back("");
    continue; // skip adding line to codetext
  }
}
@

<<Finish off reading code chunk>>=
if(curdef.empty()) {
  codetext.push_back(line + '\n');
  cerr @<< file @<< ':' @<< lineno @<< ": invalid chunk syntax\n";
  exit(1);
}
<<Finish off [[curdef]]>>
curdef = "";
@

<<[[nt-parm]] local definitions>>=
#if __cplusplus < 201100L
int stoi(const string &s)
{
  return atoi(s.c_str());
}
#endif
@

The only other directive parsed here is [[@fatal]], to abort the
process immediately.  Other utilities also process the [[@quote]]
directive, but it need not be processed here as it may only appear
within documentation chunks.

<<[[else]] process other interesting markup lines>>=
else if(!line_cmp_prefix_sp_or_end("@fatal"))
  exit(1);
@

Each [[codetext]] entry representing a definition or reference also
needs file and line number information.  For definitions, this makes
the [[-L]] option work correctly.  For references, this allows errors
to be generated correclty if parsing fails.  Rather than store this in
the same structure somehow, separate arrays ([[textfile]] and
[[textline]]) with the same length as [[codetext]] are created.  There
is no need to assign valid values for [[codetext]] lines that aren't
definition or reference lines, since they will never be used, but C++
does insist on intializing them to something (blank/0).

<<[[nt-parm]] local definitions>>=
vector<string> textfile;
vector<int> textline;
@

<<Store [[codetext]] file and line info>>=
// If slurping doc, this may have already been done
if(textfile.size() != codetext.size()) {
  textfile.insert(textfile.end(), codetext.size() - textfile.size() - 1, "");
  textline.insert(textline.end(), codetext.size() - textline.size() - 1, 0);
  textfile.push_back(file);
  textline.push_back(lineno);
}
@

<<Tack [[line]] to [[codetext.back()]]>>=
if(!incode && slurp_doc) {
  <<Store [[codetext]] file and line info>>
}
@

A separate map ([[defs]]) stores, for each unique definition name, a
list of all chunks' text array start and end indicies.  This is
stored in a single vector as two consecutive entries for each chunk.
The text itself could've been stored here instead of a global array,
reducing the need for extra integer vectors, but there is little
advantage (and some disadvantage:  the file and line info would need
to be stored here, as well) and the code is already written this way.

<<[[nt-parm]] local definitions>>=
typedef unordered_map<string, vector<int> > defmap;
defmap defs;
@

<<Store [[curdef]] info>>=
defs[curdef].push_back(codetext.size() - 1);
@

<<Finish off [[curdef]]>>=
defs[curdef].push_back(codetext.size() - 1);
@

Since normal in-line code syntax is used for parameter values, special
care must be taken to distinguish them from in-line code in definition
names.  This is done by first scanning for a definition which exactly
matches the reference.  If this is not found, parameterized
definitions are searched, with the most explicit one matching.  For
the second step, a filter is applied to replace in-line code and
parameter definitions with placeholders to create the key for a map of
stripped names ([parmdefs]]).  Each map value is a list of definition
names with the same key.  This reduces the number of chunk names that
need to be checked every time a potential expansion is found.  Note
that the list of names is in the form of a set; this is so duplicates
can be removed without extra effort.  One other possible optimization
would be to keep them in a side map until at least one chunk name with
a parameter definition is found, and then destroy the side list after
the file scan is done, but that can wait for a later version, if ever.

<<Common Parm Filter Prefix>>=
#include <set>
#if __cplusplus < 201100L
#define unordered_set set
#else
#include <unordered_set>
#endif
// for testing, it's better for strset to be in a predicatable order
// and performance just isn't that critical
typedef /*unordered_*/set<string> strset;
typedef unordered_map<string, strset> strsetmap;
@

<<[[nt-parm]] local definitions>>=
strsetmap parmdefs;
<<[[strip_chunkname]]>>
@

<<[[strip_chunkname]]>>=
// extract next parm def (@...) or val [[...]] starting at after, returning
// true if found, and false if not found.  start and after are updated to
// the start and position after parm if found, and unpredictable values
// otherwise.
static bool find_parm(const string &s, int &start, int &after)
{
  int len = s.size();
  // old code allowed just @ as a symbol, so I guess I'll do the same here
  for(; len - after >= 3; after++) {
    if(s[after] == '(' && s[after + 1] == '@') {
      start = after;
      for(after += 2; after < len; after++)
        if(s[after] == ')') {
	  ++after;
	  return true;
	}
      // old code backtracked here and continued scanning for code literals
      // but that shouldn't really have been allowed, anyway
      return false;
    }
    // No way to escape (@..), but @ escapes [[.
    if(s[after] == '@' && s[after + 1] == '[' && s[after + 2] == '[') {
      after += 2; // as per old code, but really should just be after++
      continue;
    }
    // old code ignored this entirely and tried again later if brackets weren't
    // properly nested
    // again, that shouldn't really have been allowed, anyway
    if(s[after] == '[' && s[after + 1] == '[') {
      start = after;
      int nest = 0;
      for(after += 2; len - after >= 2 &&
                      (nest || s[after] != ']' || s[after + 1] != ']');
		        after++) {
        if(s[after] == '[')
	  nest++;
	else if(nest && s[after] == ']')
	  nest--;
      }
      if(len - after < 2)
        return false;
      after += 2;
      return true;
    }
  }
  return false;
}

static string strip_chunkname(const string &s)
{
  string ret = s;
  for(int after = 0, start; find_parm(ret, start, after); after = start + 3)
    ret.replace(start, after - start, "(@)");
  return ret;
}
@

<<Store [[curdef]] info>>=
string strip = strip_chunkname(curdef);
parmdefs[strip].insert(curdef);
@

Once all has been collected, parameterized references can be resolved.
Eventually, such resolutions may result in new chunks being emitted
with parameters substituted.  Rather than do this in one step, as
previuos versions of this code did, this is done in two steps:
resolution followed by mangling and emission.

<<Mangle and dump pipeline>>=
<<Resolve missing definitions to parameterized definitions>>
@

Each resolution that requires parameter substitution causes a new
chunk to be dumped.  This chunk will be stored in [[gendefs]], indexed
by the chunk name just like [[defs]].

<<[[nt-parm]] local definitions>>=
<<[[gendefs]]>>
@

Finding missing references is done by iterating through the [[@use]]
lines.  Rather than iterating through the code array, this is done by
storing them in a map to the first reference while parsing the input,
and then iterating on the map.  It could just be a set, but if error
messages are printed, they should be printed for at least the first
reference, so its location must be known.

<<Common Parm Filter Prefix>>=
typedef unordered_map<string, int> intmap;
@

<<[[nt-parm]] local definitions>>=
intmap refs;
@

<<Store new [[@use]]>>=
if(!curdef.empty() && line[5] != '@') {
  string u = line.substr(5, line.size() - 5);
  if(refs.find(u) == refs.end())
    refs[u] = codetext.size() - 1;
}
@

Any [[@use]] which is already present in [[defs]] or [[gendefs]] is
left alone.  Otherwise, if there is a match to a parameterized name
(i.e. an entry in [[parmdefs]] for the stripped name), the best such
name is found and an entry for it is added to [[gendefs]].

<<Resolve missing definitions to parameterized definitions>>=
for(intmap::const_iterator i = refs.begin(); i != refs.end(); i++) {
  const string &use = i->first;
  if(defs.find(use) != defs.end() || gendefs.find(use) != gendefs.end())
    continue;
  string best;
  strmap best_parms;
  best = find_best_def(i->second, use, best_parms);
  if(best.empty())
    continue;
  <<Add [[best]] to [[gendef[use]]]>>
}
@

<<[[nt-parm]] local definitions>>=
<<[[find_best_def]]>>
@

<<[[find_best_def]]>>=
static string find_best_def(int loc, const string &use, strmap &best_parms)
{
  strsetmap::const_iterator pdi = parmdefs.find(strip_chunkname(use));
  if(pdi == parmdefs.end())
    return "";
  const strset &pdefs = pdi->second;
  <<Find best definition match for [[use]] in [[pdefs]]>>
}
@

Each possible expansion is checked, keeping the best.  While it's
doing that, since it already has the parameter names and their values
parsed out, a symbol table is built for the parameters.  As you may
have noticed above, the symbol table ([[best_parms]]) is returned
along with the matched chunk name.

<<Find best definition match for [[use]] in [[pdefs]]>>=
string best;
<<Prepare for finding best definition match>>
for(strset::const_iterator may = pdefs.begin(); may != pdefs.end(); may++) {
  strmap may_parms;
  <<Check [[*may]] and collect parameter defs into [[may_parms]]>>
  <<[[continue]] if [[*may]] is not best>>
  best = *may;
  best_parms = may_parms;
  <<Save additional best info about [[*may]]>>
}
<<Finish up after finding best definition match>>
return best;
@

Checking for a match involves simultaneously iterating over the
potential parameters ([[(@..)]] or [[[[..]]]]) in both names.  If the
potential parameters match, it's considered a match (even if both are
in the parameter definition form, although perhaps that should be an
error).  If they do not match exactly, they still match if the
definition side ([[*may]]) is in parameter definition form, and the
usage side ([[use]]) is in literal code text form.

<<Check [[*may]] and collect parameter defs into [[may_parms]]>>=
<<Prepare to check if definition matches>>
int mayrest = 0, urest = 0, maystart, ustart;
bool found;
while((found = find_parm(*may, maystart, mayrest))) {
  <<Prepare for resolving ambiguity per match>>
  find_parm(use, ustart, urest);
  if(!use.compare(ustart, urest - ustart, *may, maystart, mayrest - maystart))
    continue;
  if((*may)[maystart] == '[' || use[ustart] == '(')
    break; // reject
  <<Store found parameter in [[use]] in [[*may]]>>
}
if(found) // reject break from prev loop:  continue may loop
  continue;
@

<<Store found parameter in [[use]] in [[*may]]>>=
may_parms[may->substr(maystart + 1, mayrest - maystart - 2)] =
   use.substr(ustart + 2, urest - ustart - 4);
@

To resolve ambiguity, the name with the fewest substitutions is
retained.

<<Prepare for finding best definition match>>=
int best_nsub = use.size(); // Each sub is at least 3 chars, so this > max
@

<<Prepare to check if definition matches>>=
int nsub = 0;
@

<<Store found parameter in [[use]] in [[*may]]>>=
nsub++;
@

<<[[continue]] if [[*may]] is not best>>=
if(nsub > best_nsub)
  continue;
@

<<Save additional best info about [[*may]]>>=
best_nsub = nsub;
@

If two names have an equal number of substitutions, the one which has
in-line code at the first position where the two differ is chosen, and
a warning is issued.  If two names have an equal number of
substitutions, and all are in the same places, then there is an
unresolvable ambiguity, leading to error exit.

<<Prepare for finding best definition match>>=
vector<int> best_parms_loc;
vector<string> ambig;
bool bad_ambig = false;
@

<<Prepare to check if definition matches>>=
vector<int> may_parms_loc;
int parm_loc = 0;
@

<<Prepare for resolving ambiguity per match>>=
parm_loc++;
@

<<Store found parameter in [[use]] in [[*may]]>>=
may_parms_loc.push_back(parm_loc);
@

<<[[continue]] if [[*may]] is not best>>=
if(nsub == best_nsub) {
  if(!ambig.size())
    ambig.push_back(best);
  ambig.push_back(*may);
  size_t i;
  for(i = 0; i < may_parms_loc.size(); i++) {
    if(may_parms_loc[i] < best_parms_loc[i]) {
      i = ~0UL;
      break; // retain old best
    }
    if(may_parms_loc[i] > best_parms_loc[i])
      break; // choose new best
  }
  if(i == ~0UL) // retain old best and continue search
    continue;
  if((bad_ambig = i == may_parms_loc.size())) {
    best = "";
    continue;
  }
} else {
  ambig.clear();
  bad_ambig = false;
}
@

<<Save additional best info about [[*may]]>>=
best_parms_loc = may_parms_loc;
@

<<Finish up after finding best definition match>>=
if(ambig.size()) {
  if(loc >= 0)
    cerr << textfile[loc] << ':' << textline[loc] << ": ";
  if(bad_ambig)
    cerr @<< "fatal: unresolvable ";
  cerr @<< "ambiguous expansion of @<<" @<< use @<< "@>>:\n";
  for(vector<string>::const_iterator may = ambig.begin(); may != ambig.end(); may++) {
    cerr @<< "  @<<" @<< *may @<< "@>>";
    if(*may == best)
      cerr @<< " (chosen)";
    cerr @<< '\n';
  }
  if(bad_ambig) {
    cout @<< "@fatal ambiguous expansion\n";
    exit(1);
  }
}
@

Once a reference has been resolved to a chunk to be dumped, any
[[@use]] references (including unparameterized ones, due to parameter
inheritance) within that chunk must be checked as well, possibly
genererating more chunks to be dumped.

The value of the [[gendefs]] map is the text to be dumped.  Rather
than modifying the text immediately, this is stored as:

\begin{itemize}
\item The unmodified definition text of the best parameterized definition.
\item The name of the best parameterized definition, in case it should
be suppressed due to [[-r]].
\item The location of each [[@use]] in that text which is affected by
parameters.  This is the text which must be modified.
\item For each [[@use]] in that text which is affected by parameters,
the resolved value of that [[@use]].  This can be in the form of a
map, keyed on the original [[@use]] line, to avoid duplicate work and
storage.
\end{itemize}

The first two can be combined into just a reference to an iterator
into [[defs]] for the parameterized chunk.  The [[@use]] locations
aren't strictly necessary, since the dumper has to dump line-by-line,
anyway, and can just check the value list for substitutions whenever
it finds an [[@use]] line.

<<[[gendefs]]>>=
struct gendef {
  defmap::const_iterator best;
  strmap puse_val;
};
typedef unordered_map<string, gendef> genmap;
genmap gendefs;
@

The scan for affected [[@use]] lines must be done recursively to find
implicit references, so it's split into a function.  To avoid infinite
recursion, a set is also passed in to indicate references already
processed.

References (with parameter values) to parameterized chunks which
depend only on the parameters being passed in can be dumped without
changing their names.  It is not possible for the top-level reference
to be dependent on any other parameter values, since there are none.
However, any chunk which depends on inherited parameters must be
dumped with a different name, to indicate its implicit dependencies.
This new name is generated by indicating which parameters are
affecting the chunk.  To do this, the routine needs to know both the
parameters for the current chunk and the global set of parameters,
separately.  It also needs, as a return value, the set of parameters
which where affected, so it can adjust the reference as needed.

<<[[nt-parm]] local definitions>>=
<<[[set_gendef_vals]] deps>>
static strset set_gendef_vals(gendef &gd, const strmap &locals,
                              const strmap &globals = strmap(),
			      strset *processing = NULL)
{
  strset global_refs;
  strset proc;
  if(!processing)
    processing = &proc;
  const vector<int> &chunk = gd.best->second;
  for(size_t i = 0; i < chunk.size(); i += 2) {
    for(int j = chunk[i]; j <= chunk[i + 1]; j++) {
      const string &text = codetext[j];
      if(text.cmp_prefix("@use "))
        continue;
      if(gd.puse_val.find(text) != gd.puse_val.end())
        continue;
      string use = text.substr(5, text.size() - 5 - 1);
      <<Resolve [[use]] into [[gd.puse_val[text]]]>>
    }
  }
  return global_refs;
}
@

<<Add [[best]] to [[gendef[use]]]>>=
gendef &gd = gendefs[use];
gd.best = defs.find(best);
set_gendef_vals(gd, best_parms);
@

Parameter references (starting with [[@]]) are replaced with their
text if found, or are left alone (presumably to be flagged as errors
by [[notangle]] later) otherwise.

<<Resolve [[use]] into [[gd.puse_val[text]]]>>=
if(use[0] == '@') {
  strmap::const_iterator pv = locals.find(use);
  if(pv == locals.end()) {
    pv = globals.find(use);
    if(pv != globals.end())
      global_refs.insert(use);
  }
  if(pv != globals.end())
    gd.puse_val[text] = "@text " + pv->second + '\n';
} else {
  <<Resolve non-parm [[use]] into [[gd.puse_val[text]]]>>
}
@

Chunk references can have direct parameter references (via explicitly
passed-down parameters) or indirect parameter references (to be
determined recursively).  First, we'll take care of explicit
references, by substituting them into [[use]] directly.  Note that
only a single substitution is done: parameter substitutions are not
recursive, as they are in the C preprocessor.  Unknown parameters are
added as an explicit [[@use]] in order to allow [[notangle]] to
generate an error message.

Technically, this whole bit should be skipped for efficiency if there
are no possible resolutions for [[use]] (i.e. [[parmdefs[strip(use)]]]
is empty), and the individual substitutions should only occur if there
is at least one parameterized chunk with a parameter in that position,
but neither is critical for normal use, so I'll leave that as a later
exercise.

<<Resolve non-parm [[use]] into [[gd.puse_val[text]]]>>=
bool use_changed = false;
string use_err; // accumulate explicit @use refs to missing parms
int pstart, pafter = 0;
while(find_parm(use, pstart, pafter)) {
  if(use[pstart] != '[')  // actually should be an error, but I'll leave it
    continue;
  size_t vrest = pstart + 2, start;
  while((start = use.find("@<<@", vrest)) != string::npos &&
        start < (size_t)pafter) {
      // note:  old code did not accept @ escapes for <<
#if 0 // so I won't here, either
      if(use[start - 1] == '@') {
        vrest = start + 1;
        continue;
      }
#endif
    size_t end = start;
    do {
      end = use.find("@>>", end + 2);
    } while(end != string::npos && end < (size_t)pafter &&
            use[end - 1] == '@'); // but old code allowed escapes at >>...
    if(end == string::npos)
      break;
    vrest = end + 2;
    string var = use.substr(start + 2, end - (start + 2));
    strmap::const_iterator val = locals.find(var);
    if(val == locals.end()) {
      val = globals.find(var);
      if(val != globals.end())
        global_refs.insert(var);
    }
    if(val == globals.end()) {
      // generate an undefined reference error; prefixed to use
      use_err += var + "\n@use ";
      use_changed = true;
      continue;
    }
    if(val->second != var) {
      use_changed = true;
      use.replace(start, end + 2 - start, val->second);
      int chg = (end - start + 2) - val->second.size();
      pafter -= chg;
      vrest -= chg;
    }
  }
}
if(!use_err.empty()) {
  gd.puse_val[text] = use_err + use;
  continue;
}
@

Just like for regular definition lookups, parameterized references
must then be resolved to the best match before recursion.  The local
parameters for recursion will be [[best_parms]].

<<Resolve non-parm [[use]] into [[gd.puse_val[text]]]>>=
strmap best_parms;
string best = find_best_def(i, use, best_parms);
@

The recursive call's global parameters are the current globals, merged
with the current locals.  This merging should only be done once per
function call for efficiency, but it's not that important.

<<Resolve non-parm [[use]] into [[gd.puse_val[text]]]>>=
strmap new_globals = globals;
for(strmap::const_iterator it = locals.begin(); it != locals.end(); it++)
  new_globals[it->first] = it->second;
@

Finally, name mangling for implicit inheritance needs to be dealt
with.  The final name of the reference (and its associated generated
chunk) will be made unique if inherited parameters are involved.
Uniqueness is provided by appending an unlikely separator ([[@<<>>]])
and a unique string.  That unique string could be a number of things:
the root chunk name which defines the parameters, a random unique
string, or the set of parameters and values which affect it.  Older
versions of this code used the former.  The latter is the best way to
share code, so that is what I will use now.  For recursion limiting,
the key assumes all globals will affect the result.

<<Common Parm Filter Prefix>>=
#include <algorithm>
@

<<[[set_gendef_vals]] deps>>=
static const string &name_of(strset::const_iterator &it) { return *it; }
static const string &name_of(strmap::const_iterator &it) { return it->first; }
template <class T> static void append_str(string &str, strmap syms, T which)
{
  vector<string> vals;
  for(typename T::const_iterator it = which.begin(); it != which.end(); it++)
    vals.push_back(name_of(it) + "[[" + syms[name_of(it)] + "]]");
  sort(vals.begin(), vals.end());
  str += "@<<>>";
  for(size_t i = 0; i < vals.size(); i++)
    str += vals[i];
}
@

<<Resolve non-parm [[use]] into [[gd.puse_val[text]]]>>=
string rkey = use;
append_str(rkey, new_globals, new_globals);
if(processing->find(rkey) != processing->end())
  continue;
processing->insert(rkey);
gendef ngd;
ngd.best = defs.find(best);
strset refs = set_gendef_vals(ngd, best_parms, new_globals, processing);
if(refs.size()) {
  append_str(use, new_globals, refs);
  use_changed = true;
  for(strset::const_iterator it = refs.begin(); it != refs.end(); it++)
    if(locals.find(*it) != locals.end())
      global_refs.insert(*it);
}
if(use_changed)
  gd.puse_val[text] = "@use " + use + '\n';
if(gendefs.find(use) == gendefs.end() &&
   defs.find(use) == defs.end())
  gendefs[use] = ngd;
@

Now that the generated chunks have been defined, it's time to trim the
list if [[-r]] was specified.  This is done by performing a function
similar to [[noroots]] on all chunks, and removing them if they are
either are generated or they are the source for a generated chunk.
This is repeated until there are no new roots.

<<Mangle and dump pipeline>>=
if(rmunref) {
  <<Collect [[@use]] for [[-r]]>>
  <<Remove roots that generate or are generated>>
}
@

To improve performance, instead of tagging every [[@use]] with just a
flag, it's tagged with a set of referrers.  This means that when a
chunk is removed, it can be removed from the referrer lists as well.
Once the referrer list is empty, it is untagged.  In fact, in order to
avoid all rescans for [[@use]], another map is generated from the
chunk name to all its uses as a set.

<<Collect [[@use]] for [[-r]]>>=
strsetmap curuse, defuse;
for(defmap::const_iterator def = defs.begin(); def != defs.end(); def++)
  for(size_t i = 0; i < def->second.size(); i += 2)
    for(int j = def->second[i]; j <= def->second[i + 1]; j++) {
      const string &u = codetext[j];
      if(!u.cmp_prefix("@use ") && u[5] != '@') {
        curuse[u].insert(def->first);
        defuse[def->first].insert(u);
      }
    }
for(genmap::const_iterator gen = gendefs.begin(); gen != gendefs.end(); gen++) {
  const vector<int> &def = gen->second.best->second;
  for(size_t i = 0; i < def.size(); i += 2)
    for(int j = def[i]; j <= def[i + 1]; j++) {
      const string *u = &codetext[j];
      if(u->cmp_prefix("@use ") || (*u)[5] == '@')
        continue;
      strmap::const_iterator v = gen->second.puse_val.find(*u);
      if(v != gen->second.puse_val.end()) {
        u = &v->second;
        if((*u)[5] == '@') // if there was an error passing down parms
          continue;
      }
      curuse[*u].insert(gen->first);
      defuse[gen->first].insert(*u);
    }
}
@

This means that only removals are done in a loop, rather than
rescanning every time.  However, there are a few problems with
deleting map entries.  First, the loop has to be restarted since the
iterator may become invalid.  Second, for the same reason, [[defs]]
cannot be deleted, since iterators are stored in [[gendefs]].  For
these reasons, instead of deleting the entries, they are added to a
blacklist.

<<[[nt-parm]] local definitions>>=
strset blacklist;
@

<<Remove roots that generate or are generated>>=
bool didone;
do {
  didone = false;
  for(defmap::iterator def = defs.begin(); def != defs.end(); def++)
    if(blacklist.find(def->first) == blacklist.end() &&
       rmroot(def->first, curuse, defuse, false)) {
      blacklist.insert(def->first);
      didone = true;
    }
  for(genmap::iterator gen = gendefs.begin(); gen != gendefs.end(); gen++)
    if(blacklist.find(gen->first) == blacklist.end() &&
       rmroot(gen->first, curuse, defuse, true)) {
      blacklist.insert(gen->first);
      didone = true;
    }
} while(didone);
@

<<[[nt-parm]] local definitions>>=
static bool rmroot(const string &def, strsetmap &curuse, strsetmap &defuse,
                   bool isgen)
{
  if(curuse.find("@use " + def + '\n') != curuse.end())
    return false;
  if(!isgen) {
    bool found = false;
    for(genmap::const_iterator gen = gendefs.begin(); gen != gendefs.end(); gen++)
      if((found = gen->second.best->first == def))
        break;
    if(!found)
      return false;
  }
  const strset &uses = defuse[def];
  for(strset::const_iterator it = uses.begin(); it != uses.end(); it++) {
    curuse[*it].erase(def);
    if(curuse[*it].empty())
      curuse.erase(*it);
  }
  defuse.erase(def);
  return true;
}
@

Now it's finally time to actually dump the chunks.  If [[-r]] was
specified, then all chunks, including those in [[defs]], must be
dumped.  Otherwise, just the [[gendefs]] ones.  It doesn't really
matter what order they're dumped in, just that all chunks for the same
definition are dumped in the original order.

The definition text to be dumped consists of all chunks for the
original definition.  Before each chunk's text emission, a [[@file]]
and [[@line]] directive is emitted, to keep [[-L]] working.  For
[[defs]] chunks, nothing else needs to be dumped.  For [[gendefs]]
chunks, the first line of each chunk, which is always the [[@defn]]
line, needs to have the old name replaced with the new.  Also, any
[[@use]] line needs to be replaced by its [[puse_val]] text, if
present.

<<Mangle and dump pipeline>>=
if(rmunref)
  for(defmap::const_iterator def = defs.begin(); def != defs.end(); def++)
    if(blacklist.find(def->first) == blacklist.end())
      dump_chunk(def);
for(genmap::const_iterator gen = gendefs.begin(); gen != gendefs.end(); gen++)
  if(blacklist.find(gen->first) == blacklist.end())
    dump_chunk(gen->second.best, &gen->first, &gen->second.puse_val);
@

<<[[nt-parm]] local definitions>>=
static void dump_chunk(defmap::const_iterator def, const string *gen = NULL,
                       const strmap *parms = NULL)
{
  for(size_t i = 0; i < def->second.size(); i += 2) {
    int j = def->second[i], last = def->second[i + 1];
    cout << "@file " << textfile[j];
    cout << "\n@line " << textline[j] + 1 << '\n';
    if(gen) {
      cout << codetext[j].substr(0, codetext[j].size() - (def->first.size() + 1));
      cout << *gen + '\n';
      j++;
    }
    for(; j <= last; j++) {
      const string &t = codetext[j];
      if(gen && !t.cmp_prefix("@use ")) {
        strmap::const_iterator v = parms->find(t);
	if(v != parms->end()) {
	  cout << v->second;
	  continue;
	}
      }
      cout << t;
    }
  }
}
@

\section{Weaving}

These enhancements also require changes to the weavers.  No particular
effort will be made to typeset things nicely.  Parameter references
should be typeset as chunk references, but without the links (where
would they link to?).  To do that, they need to be hidden from the
indexer, and unhidden after the indexer.  Likewise, to get cross
references right, references to parameterized chunks with actual
parameters need to be converted to the definition format for the
indexer, and then changed back afterwards.  These tasks can be
accomplished by two filters:  one for before indexing, and one for
after.

<<nw-parm-preidx.cpp>>=
<<Common Parm Filter Prefix>>

@

<<nw-parm-postidx.cpp>>=
<<Common Parm Filter Prefix>>

@

The pre-index filter needs to know how to interpret chunk references
with in-line code:  either as in-line code, or as macro parameters.
The only way to do this is to know the names of all chunks.  Since
weaving does not require all tangling inputs, but this particular task
does require them, any inputs not included in the weaving process must
be specified on the filter's command line.  While it might be nice to
use the standard markup tool for this, it is not present in noweb 3.
Instead, the file is parsed directly, with a very limited view of what
constitutes the start of a chunk.  This probably needs improvement.

Much like the tangler's reader, each definition is placed into two
arrays:  one indexed by its real name, and one indexed by its name
with all parameter definitions and references stripped out.  That way,
finding the actual chunk to use for a macro reference is easier.

<<nw-parm-preidx.cpp>>=
<<[[nw-parm-preidx]] locals>>

int main(int argc, const char **argv)
{
  <<nw-parm-preidx>>
  return 0;
}
@

<<[[nw-parm-preidx]] locals>>=
strsetmap parmdefs;
@

<<nw-parm-preidx>>=
strset defs;

for(int i = 1; i < argc; i++) {
#if 0
  fh = popen("markup " + argv[i]);  // C++ doesn't have any such function
#else
  ifstream fh(argv[i]);
#endif
  string line;
  while(getline(fh, line).good() || line.size()) {
#if 0
    if(line.cmp_prefix("@defn "))
      continue;
    string chunkname = line.substr(6);
#else
    if(line.cmp_prefix("@<<") ||
       line.substr(line.size() - 3, 3) != "@>>=")
      continue;
    string chunkname = line.substr(2, line.size() - 3);
#endif
    defs.insert(chunkname);
    string s = strip_chunkname(chunkname);
    parmdefs[s].insert(chunkname);
  }
  fh.close();
}
@

<<Common Parm Filter Prefix>>=
#include <fstream>
@

<<[[nw-parm-preidx]] locals>>=
<<[[strip_chunkname]]>>
@

The filter process needs to scan the file more than once.  The first
time, it gathers definitions just like it did for the command-line
arguments.  For convenience, the file is just read into an array of
plain text lines for processing.

<<nw-parm-preidx>>=
vector<string> file;
string line;
while(getline(cin, line).good() || line.size())
  file.push_back(line);
for(size_t i = 0; i < file.size(); i++) {
  if(file[i].cmp_prefix("@defn "))
    continue;
  string chunkname = file[i].substr(6);
  defs.insert(chunkname);
  string s = strip_chunkname(chunkname);
  parmdefs[s].insert(chunkname);
}
@

For the second pass, an attempt is made to match a chunk reference
with a definition.  If it matches, and requires parameter expansion to
do so, its name is replaced with the parameterized definition, and its
old name is saved using [[@nwparmcall]].  The matching method is
copied from the tangler, with minor differences.  Line and file
information are not tracked by this filter, so the location string is 
always blank.  Also, since a standalone chunk does not have parameter
values, the specialization performed after parameter expansion in chunk
names can't be done.  Unfortunately, there is also no special link
between a parameterized chunk and all of its potential specializations.

Parameter references are simply hidden by renaming them to
[[@nwparmuse]].

As a special hack, in order to highlight parameters better, an extra
set of square brackets is placed around them.  This repeats some of
the work that [[find_best_def]] does, but that's not too terrible.

<<nw-parm-preidx>>=
for(size_t i = 0; i < file.size(); i++) {
  int j;
  if(!file[i].cmp_prefix("@use ") && file[i][5] != '@' &&
     (j = file[i].find("@[[")) && file[i].find("@]]", j + 2)) {
    string chunkname = file[i].substr(5);
    strmap best_parms;
    string best = find_best_def(-1, chunkname, best_parms);
    if(!best.empty() && best != chunkname) {
      string cur = "";
      int ca = 0, cp, ba = 0, bp;
      while(find_parm(best, bp, ba)) {
        int oca = ca;
        find_parm(chunkname, cp, ca);
	cur += chunkname.substr(oca, cp - oca);
	if(!best.compare(bp, ba - bp, chunkname, cp, ca - cp))
	  cur += chunkname.substr(cp, ca - cp);
	else
	  cur += "[[" + chunkname.substr(cp, ca - cp) + "]]";
      }
      cout << "@nwparmcall " << cur << chunkname.substr(ca) << "\n"
              "@use " << best << '\n';
    } else
      cout << file[i] << '\n';
  } else {
    if(!file[i].cmp_prefix("@use @"))
      cout << "@nwparm" << file[i].substr(1);
    else
      cout << file[i];
    cout << '\n';
  }
}
@

<<[[nw-parm-preidx]] locals>>=
vector<string> textfile;
vector<int> textline;
<<[[find_best_def]]>>
@

For the third pass, done after the index, the above-added tags are
reverted.

<<nw-parm-postidx.cpp>>=
int main(void)
{
  string nwparmcall;
  string line;
  while(getline(cin, line).good() || line.size()) {
    if(!line.cmp_prefix("@nwparmcall "))
      nwparmcall = line.substr(12);
    else if(!nwparmcall.empty() && !line.cmp_prefix("@use ")) {
      cout << "@use " << nwparmcall << '\n';
      nwparmcall = "";
    } else if(!line.cmp_prefix("@nwparmuse "))
      cout << '@' << line.substr(7) << '\n';
    else
      cout << line << '\n';
  }
}
@

\section{Other}

This is not the end of it: [[noroots]] (noweb-2 only) needs changes as
well.  However, rather than re-implement this from scratch, as older
versions of this code did, I will simply implement it by editing the
original script.  This would be much easier, of course, if the
original script took filter arguments like [[notangle]] and
[[noweave]], but we can just create a new script from the old,
inserting [[nt-parm]] into the pipeline:

<<mk-noroots>>=
sed 's~| awk~| ${NT_PARM_LOC:-${0%/*}}/nt-parm -r &~' <`which noroots`
@

The only issue is that it behaves erratically on errors.  This is not
a big deal, though.  At least not big enough to want to re-implement
from scratch.

\section{Usage}

In summary, to use this, see the introduction for syntax.  Extract the
following three code roots from this document, compile them with your C++
compiler into an executable, and use as described:

\begin{itemize}
\item [[nt-parm.cpp]] --- always use this as a filter when tangling.
Add [[-r]] to suppress parameterized chunks that are only indirectly
referenced.  Add [[-d]] to include the preceeding documentation chunk
with generated code chunks.
\item [[nw-parm-preidx.cpp]] --- always use this as a filter when weaving,
before indexing (or implicit indexing, such [[-x]], [[-index]], etc.).
\item [[nw-parm-postidx.cpp]] --- always use this as a filter when
weaving, after indexing (or implicit indexing).  If no indexing is
done at all, use both of these filters with nothing in between.
\end{itemize}

For example:

To compile on UNIX:
\begin{verbatim}
for x in nt-prm nw-parm-{pre,post}idx; do
  notangle -L -R${x}.cpp parm.nw > ${x}.cpp
  c++ -O -o ${x} ${x.cpp}
done
\end{verbatim}

Then, if desired, make a [[noroots]] replacement:

\begin{verbatim}
notangle -Rmk-noroots parm.nw | sh > noroots-parm
chmod +x noroots-parm
\end{verbatim}

To use on UNIX, if the executables are in the current directory:
\begin{verbatim}
notangle -filter ./nt-parm -R'myroot' mystuff.nw > out
noweave -filter ./nw-parm-preidx -index \
        -filter ./nw-parm-postidx mystuff.nw > mystuff.tex
noroots-parm mystuff.nw
\end{verbatim}

If you are weaving a file that is meant to be tangled together with
other files, those other files need to be given on the
[[nw-parm-preidx]] command line.  Repeating the main input file is
harmless.  For example, if [[x.nw]] and [[y.nw]] are normally tangled
together, but weaved separately (again, on UNIX, if the executables
are in the current directory):

\begin{verbatim}
notangle -filter ./nt-parm -R'myroot' x.nw y.nw > out
nountangle -filter ./nt-parm\ -d -R'myroot' x.nw y.nw > out
noweave -filter "./nw-parm-preidx y.nw" -index \
        -filter ./nw-parm-postidx x.nw > x.tex
noweave -filter "./nw-parm-preidx x.nw" -index \
        -filter ./nw-parm-postidx y.nw > y.tex
\end{verbatim}

\section{Index}

\nowebchunks

% note: no identifier indexing is done right now

%\nowebindex

\end{document}

% For some reason, having this causes chunk index to fail
%@<<Known Data Types>>=
%vector,set,map,unordered_map,unordered_set,string,std,stringstream,%
%const_iterator,%
%strmap,strset,strsetmap,intmap,defmap,genmap,%
%@
